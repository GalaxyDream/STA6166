{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id_number', 'psa_level', 'cancer_volume', 'weight', 'age', \n",
    "                'benign_prostatic_hyperplasia', 'seminal_vesicle_invasion', \n",
    "                'capsular_penetration', 'gleason_score']\n",
    "num_vars = ['cancer_volume', 'weight', 'age', \n",
    "            'benign_prostatic_hyperplasia',  \n",
    "            'capsular_penetration', 'gleason_score']\n",
    "cat_vars = ['seminal_vesicle_invasion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../resources/assignment3_1_cpy.txt', sep = ',', header = None, names = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>psa_level</th>\n",
       "      <th>cancer_volume</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>benign_prostatic_hyperplasia</th>\n",
       "      <th>seminal_vesicle_invasion</th>\n",
       "      <th>capsular_penetration</th>\n",
       "      <th>gleason_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.730134</td>\n",
       "      <td>-1.373472e-17</td>\n",
       "      <td>-1.422116e-16</td>\n",
       "      <td>3.960177e-16</td>\n",
       "      <td>-5.894483e-17</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>-9.156479e-18</td>\n",
       "      <td>-6.066167e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.782925</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.413995</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.651000</td>\n",
       "      <td>-8.551700e-01</td>\n",
       "      <td>-7.612804e-01</td>\n",
       "      <td>-3.071272e+00</td>\n",
       "      <td>-8.362184e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.934898e-01</td>\n",
       "      <td>-1.184784e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.641000</td>\n",
       "      <td>-6.767506e-01</td>\n",
       "      <td>-3.527041e-01</td>\n",
       "      <td>-5.192637e-01</td>\n",
       "      <td>-8.362184e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.934898e-01</td>\n",
       "      <td>-1.184784e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.330000</td>\n",
       "      <td>-3.471169e-01</td>\n",
       "      <td>-1.783908e-01</td>\n",
       "      <td>1.523174e-01</td>\n",
       "      <td>-3.908796e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.747319e-01</td>\n",
       "      <td>1.672636e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.328000</td>\n",
       "      <td>1.797032e-01</td>\n",
       "      <td>6.416444e-02</td>\n",
       "      <td>5.552660e-01</td>\n",
       "      <td>7.337335e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.667051e-01</td>\n",
       "      <td>1.672636e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>265.072000</td>\n",
       "      <td>4.898637e+00</td>\n",
       "      <td>8.857831e+00</td>\n",
       "      <td>2.032744e+00</td>\n",
       "      <td>2.554512e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.210243e+00</td>\n",
       "      <td>1.519311e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        psa_level  cancer_volume        weight           age  \\\n",
       "count   97.000000   9.700000e+01  9.700000e+01  9.700000e+01   \n",
       "mean    23.730134  -1.373472e-17 -1.422116e-16  3.960177e-16   \n",
       "std     40.782925   1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min      0.651000  -8.551700e-01 -7.612804e-01 -3.071272e+00   \n",
       "25%      5.641000  -6.767506e-01 -3.527041e-01 -5.192637e-01   \n",
       "50%     13.330000  -3.471169e-01 -1.783908e-01  1.523174e-01   \n",
       "75%     21.328000   1.797032e-01  6.416444e-02  5.552660e-01   \n",
       "max    265.072000   4.898637e+00  8.857831e+00  2.032744e+00   \n",
       "\n",
       "       benign_prostatic_hyperplasia  seminal_vesicle_invasion  \\\n",
       "count                  9.700000e+01                 97.000000   \n",
       "mean                  -5.894483e-17                  0.216495   \n",
       "std                    1.000000e+00                  0.413995   \n",
       "min                   -8.362184e-01                  0.000000   \n",
       "25%                   -8.362184e-01                  0.000000   \n",
       "50%                   -3.908796e-01                  0.000000   \n",
       "75%                    7.337335e-01                  0.000000   \n",
       "max                    2.554512e+00                  1.000000   \n",
       "\n",
       "       capsular_penetration  gleason_score  \n",
       "count          9.700000e+01   9.700000e+01  \n",
       "mean          -9.156479e-18  -6.066167e-16  \n",
       "std            1.000000e+00   1.000000e+00  \n",
       "min           -5.934898e-01  -1.184784e+00  \n",
       "25%           -5.934898e-01  -1.184784e+00  \n",
       "50%           -4.747319e-01   1.672636e-01  \n",
       "75%            2.667051e-01   1.672636e-01  \n",
       "max            4.210243e+00   1.519311e+00  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rescaled[['psa_level', 'cancer_volume', 'weight', 'age', 'benign_prostatic_hyperplasia', 'seminal_vesicle_invasion', 'capsular_penetration', 'gleason_score']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_number</th>\n",
       "      <th>psa_level</th>\n",
       "      <th>cancer_volume</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>benign_prostatic_hyperplasia</th>\n",
       "      <th>seminal_vesicle_invasion</th>\n",
       "      <th>capsular_penetration</th>\n",
       "      <th>gleason_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_number</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602684</td>\n",
       "      <td>0.620998</td>\n",
       "      <td>0.113741</td>\n",
       "      <td>0.196556</td>\n",
       "      <td>0.165005</td>\n",
       "      <td>0.566780</td>\n",
       "      <td>0.476752</td>\n",
       "      <td>0.537924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psa_level</th>\n",
       "      <td>0.602684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.624151</td>\n",
       "      <td>0.026213</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>-0.016486</td>\n",
       "      <td>0.528619</td>\n",
       "      <td>0.550793</td>\n",
       "      <td>0.429580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer_volume</th>\n",
       "      <td>0.620998</td>\n",
       "      <td>0.624151</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>0.039094</td>\n",
       "      <td>-0.133209</td>\n",
       "      <td>0.581742</td>\n",
       "      <td>0.692897</td>\n",
       "      <td>0.481438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>0.113741</td>\n",
       "      <td>0.026213</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164324</td>\n",
       "      <td>0.321849</td>\n",
       "      <td>-0.002410</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>-0.024207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.196556</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>0.039094</td>\n",
       "      <td>0.164324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.366341</td>\n",
       "      <td>0.117658</td>\n",
       "      <td>0.099555</td>\n",
       "      <td>0.225852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benign_prostatic_hyperplasia</th>\n",
       "      <td>0.165005</td>\n",
       "      <td>-0.016486</td>\n",
       "      <td>-0.133209</td>\n",
       "      <td>0.321849</td>\n",
       "      <td>0.366341</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119553</td>\n",
       "      <td>-0.083009</td>\n",
       "      <td>0.026826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seminal_vesicle_invasion</th>\n",
       "      <td>0.566780</td>\n",
       "      <td>0.528619</td>\n",
       "      <td>0.581742</td>\n",
       "      <td>-0.002410</td>\n",
       "      <td>0.117658</td>\n",
       "      <td>-0.119553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.680284</td>\n",
       "      <td>0.428573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capsular_penetration</th>\n",
       "      <td>0.476752</td>\n",
       "      <td>0.550793</td>\n",
       "      <td>0.692897</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.099555</td>\n",
       "      <td>-0.083009</td>\n",
       "      <td>0.680284</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gleason_score</th>\n",
       "      <td>0.537924</td>\n",
       "      <td>0.429580</td>\n",
       "      <td>0.481438</td>\n",
       "      <td>-0.024207</td>\n",
       "      <td>0.225852</td>\n",
       "      <td>0.026826</td>\n",
       "      <td>0.428573</td>\n",
       "      <td>0.461566</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id_number  psa_level  cancer_volume    weight  \\\n",
       "id_number                      1.000000   0.602684       0.620998  0.113741   \n",
       "psa_level                      0.602684   1.000000       0.624151  0.026213   \n",
       "cancer_volume                  0.620998   0.624151       1.000000  0.005107   \n",
       "weight                         0.113741   0.026213       0.005107  1.000000   \n",
       "age                            0.196556   0.017199       0.039094  0.164324   \n",
       "benign_prostatic_hyperplasia   0.165005  -0.016486      -0.133209  0.321849   \n",
       "seminal_vesicle_invasion       0.566780   0.528619       0.581742 -0.002410   \n",
       "capsular_penetration           0.476752   0.550793       0.692897  0.001579   \n",
       "gleason_score                  0.537924   0.429580       0.481438 -0.024207   \n",
       "\n",
       "                                   age  benign_prostatic_hyperplasia  \\\n",
       "id_number                     0.196556                      0.165005   \n",
       "psa_level                     0.017199                     -0.016486   \n",
       "cancer_volume                 0.039094                     -0.133209   \n",
       "weight                        0.164324                      0.321849   \n",
       "age                           1.000000                      0.366341   \n",
       "benign_prostatic_hyperplasia  0.366341                      1.000000   \n",
       "seminal_vesicle_invasion      0.117658                     -0.119553   \n",
       "capsular_penetration          0.099555                     -0.083009   \n",
       "gleason_score                 0.225852                      0.026826   \n",
       "\n",
       "                              seminal_vesicle_invasion  capsular_penetration  \\\n",
       "id_number                                     0.566780              0.476752   \n",
       "psa_level                                     0.528619              0.550793   \n",
       "cancer_volume                                 0.581742              0.692897   \n",
       "weight                                       -0.002410              0.001579   \n",
       "age                                           0.117658              0.099555   \n",
       "benign_prostatic_hyperplasia                 -0.119553             -0.083009   \n",
       "seminal_vesicle_invasion                      1.000000              0.680284   \n",
       "capsular_penetration                          0.680284              1.000000   \n",
       "gleason_score                                 0.428573              0.461566   \n",
       "\n",
       "                              gleason_score  \n",
       "id_number                          0.537924  \n",
       "psa_level                          0.429580  \n",
       "cancer_volume                      0.481438  \n",
       "weight                            -0.024207  \n",
       "age                                0.225852  \n",
       "benign_prostatic_hyperplasia       0.026826  \n",
       "seminal_vesicle_invasion           0.428573  \n",
       "capsular_penetration               0.461566  \n",
       "gleason_score                      1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_number</th>\n",
       "      <th>psa_level</th>\n",
       "      <th>cancer_volume</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>benign_prostatic_hyperplasia</th>\n",
       "      <th>seminal_vesicle_invasion</th>\n",
       "      <th>capsular_penetration</th>\n",
       "      <th>gleason_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.5599</td>\n",
       "      <td>15.959</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.3716</td>\n",
       "      <td>27.660</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.6005</td>\n",
       "      <td>14.732</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>26.576</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.448</td>\n",
       "      <td>2.1170</td>\n",
       "      <td>30.877</td>\n",
       "      <td>62</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2.160</td>\n",
       "      <td>0.3499</td>\n",
       "      <td>25.280</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2.160</td>\n",
       "      <td>2.0959</td>\n",
       "      <td>32.137</td>\n",
       "      <td>64</td>\n",
       "      <td>1.8589</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2.340</td>\n",
       "      <td>1.9937</td>\n",
       "      <td>34.467</td>\n",
       "      <td>58</td>\n",
       "      <td>4.6646</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2.858</td>\n",
       "      <td>0.4584</td>\n",
       "      <td>34.467</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2.858</td>\n",
       "      <td>1.2461</td>\n",
       "      <td>25.534</td>\n",
       "      <td>63</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>3.561</td>\n",
       "      <td>1.2840</td>\n",
       "      <td>36.598</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>3.561</td>\n",
       "      <td>0.2592</td>\n",
       "      <td>36.598</td>\n",
       "      <td>63</td>\n",
       "      <td>3.5609</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>3.561</td>\n",
       "      <td>5.0028</td>\n",
       "      <td>20.491</td>\n",
       "      <td>63</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5488</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3.857</td>\n",
       "      <td>4.3929</td>\n",
       "      <td>20.086</td>\n",
       "      <td>67</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>4.055</td>\n",
       "      <td>3.3535</td>\n",
       "      <td>31.187</td>\n",
       "      <td>57</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>4.263</td>\n",
       "      <td>4.6646</td>\n",
       "      <td>21.328</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>4.349</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>33.784</td>\n",
       "      <td>70</td>\n",
       "      <td>3.4556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5488</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>4.437</td>\n",
       "      <td>9.8749</td>\n",
       "      <td>38.475</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4477</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>4.759</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>26.311</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>4.953</td>\n",
       "      <td>1.1972</td>\n",
       "      <td>46.063</td>\n",
       "      <td>70</td>\n",
       "      <td>5.2593</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>5.155</td>\n",
       "      <td>3.1582</td>\n",
       "      <td>30.569</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>5.259</td>\n",
       "      <td>7.8460</td>\n",
       "      <td>33.115</td>\n",
       "      <td>60</td>\n",
       "      <td>4.3492</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8574</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>5.474</td>\n",
       "      <td>0.5827</td>\n",
       "      <td>29.371</td>\n",
       "      <td>59</td>\n",
       "      <td>0.4493</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>5.529</td>\n",
       "      <td>5.9299</td>\n",
       "      <td>31.500</td>\n",
       "      <td>63</td>\n",
       "      <td>1.5527</td>\n",
       "      <td>0</td>\n",
       "      <td>3.2544</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>5.641</td>\n",
       "      <td>1.4770</td>\n",
       "      <td>39.252</td>\n",
       "      <td>69</td>\n",
       "      <td>4.9530</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>5.871</td>\n",
       "      <td>4.2631</td>\n",
       "      <td>22.646</td>\n",
       "      <td>68</td>\n",
       "      <td>1.3499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>6.050</td>\n",
       "      <td>1.6653</td>\n",
       "      <td>41.264</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4493</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>6.172</td>\n",
       "      <td>0.6703</td>\n",
       "      <td>47.942</td>\n",
       "      <td>67</td>\n",
       "      <td>6.1719</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>6.360</td>\n",
       "      <td>2.8292</td>\n",
       "      <td>22.874</td>\n",
       "      <td>67</td>\n",
       "      <td>1.2461</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0513</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>6.619</td>\n",
       "      <td>11.1340</td>\n",
       "      <td>29.371</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0531</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>19.298</td>\n",
       "      <td>9.0250</td>\n",
       "      <td>57.397</td>\n",
       "      <td>72</td>\n",
       "      <td>10.0744</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>19.298</td>\n",
       "      <td>0.6376</td>\n",
       "      <td>82.269</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>19.492</td>\n",
       "      <td>3.2871</td>\n",
       "      <td>119.104</td>\n",
       "      <td>72</td>\n",
       "      <td>10.2779</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4493</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>20.287</td>\n",
       "      <td>6.4237</td>\n",
       "      <td>36.234</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.7434</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>20.905</td>\n",
       "      <td>3.1899</td>\n",
       "      <td>28.219</td>\n",
       "      <td>77</td>\n",
       "      <td>5.7546</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>21.328</td>\n",
       "      <td>3.3535</td>\n",
       "      <td>46.063</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2461</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>21.758</td>\n",
       "      <td>6.2965</td>\n",
       "      <td>25.534</td>\n",
       "      <td>60</td>\n",
       "      <td>1.5527</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2544</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>26.576</td>\n",
       "      <td>20.0855</td>\n",
       "      <td>46.993</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>6.7531</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>28.219</td>\n",
       "      <td>23.1039</td>\n",
       "      <td>26.050</td>\n",
       "      <td>68</td>\n",
       "      <td>0.9512</td>\n",
       "      <td>1</td>\n",
       "      <td>11.2459</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>29.666</td>\n",
       "      <td>7.4633</td>\n",
       "      <td>83.931</td>\n",
       "      <td>72</td>\n",
       "      <td>8.3311</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6487</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>31.187</td>\n",
       "      <td>12.6797</td>\n",
       "      <td>77.478</td>\n",
       "      <td>78</td>\n",
       "      <td>10.2779</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>31.817</td>\n",
       "      <td>14.1540</td>\n",
       "      <td>35.874</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>13.1971</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>33.448</td>\n",
       "      <td>16.1190</td>\n",
       "      <td>45.604</td>\n",
       "      <td>63</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4477</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>33.784</td>\n",
       "      <td>4.3492</td>\n",
       "      <td>21.542</td>\n",
       "      <td>66</td>\n",
       "      <td>1.7507</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2461</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>34.124</td>\n",
       "      <td>12.3049</td>\n",
       "      <td>32.137</td>\n",
       "      <td>57</td>\n",
       "      <td>1.5527</td>\n",
       "      <td>0</td>\n",
       "      <td>10.2779</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>35.517</td>\n",
       "      <td>13.5991</td>\n",
       "      <td>48.911</td>\n",
       "      <td>77</td>\n",
       "      <td>0.5886</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7507</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>35.517</td>\n",
       "      <td>14.5851</td>\n",
       "      <td>46.525</td>\n",
       "      <td>65</td>\n",
       "      <td>3.0649</td>\n",
       "      <td>0</td>\n",
       "      <td>5.7546</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>36.234</td>\n",
       "      <td>4.7588</td>\n",
       "      <td>40.854</td>\n",
       "      <td>60</td>\n",
       "      <td>5.4739</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2479</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>37.713</td>\n",
       "      <td>27.1126</td>\n",
       "      <td>33.784</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.2779</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>39.646</td>\n",
       "      <td>7.5383</td>\n",
       "      <td>41.679</td>\n",
       "      <td>58</td>\n",
       "      <td>5.1552</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>40.854</td>\n",
       "      <td>5.6407</td>\n",
       "      <td>29.079</td>\n",
       "      <td>62</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3499</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>53.517</td>\n",
       "      <td>16.6099</td>\n",
       "      <td>112.168</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7048</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>54.055</td>\n",
       "      <td>4.7588</td>\n",
       "      <td>40.447</td>\n",
       "      <td>76</td>\n",
       "      <td>2.5600</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2479</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>56.261</td>\n",
       "      <td>25.7903</td>\n",
       "      <td>60.340</td>\n",
       "      <td>68</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>62.178</td>\n",
       "      <td>12.5535</td>\n",
       "      <td>39.646</td>\n",
       "      <td>61</td>\n",
       "      <td>3.8574</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>80.640</td>\n",
       "      <td>16.9455</td>\n",
       "      <td>48.424</td>\n",
       "      <td>68</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.7434</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>107.770</td>\n",
       "      <td>45.6042</td>\n",
       "      <td>49.402</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>8.7583</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>170.716</td>\n",
       "      <td>18.3568</td>\n",
       "      <td>29.964</td>\n",
       "      <td>52</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7048</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>239.847</td>\n",
       "      <td>17.8143</td>\n",
       "      <td>43.380</td>\n",
       "      <td>68</td>\n",
       "      <td>4.7588</td>\n",
       "      <td>1</td>\n",
       "      <td>4.7588</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>265.072</td>\n",
       "      <td>32.1367</td>\n",
       "      <td>52.985</td>\n",
       "      <td>68</td>\n",
       "      <td>1.5527</td>\n",
       "      <td>1</td>\n",
       "      <td>18.1741</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_number  psa_level  cancer_volume   weight  age  \\\n",
       "0           1      0.651         0.5599   15.959   50   \n",
       "1           2      0.852         0.3716   27.660   58   \n",
       "2           3      0.852         0.6005   14.732   74   \n",
       "3           4      0.852         0.3012   26.576   58   \n",
       "4           5      1.448         2.1170   30.877   62   \n",
       "5           6      2.160         0.3499   25.280   50   \n",
       "6           7      2.160         2.0959   32.137   64   \n",
       "7           8      2.340         1.9937   34.467   58   \n",
       "8           9      2.858         0.4584   34.467   47   \n",
       "9          10      2.858         1.2461   25.534   63   \n",
       "10         11      3.561         1.2840   36.598   65   \n",
       "11         12      3.561         0.2592   36.598   63   \n",
       "12         13      3.561         5.0028   20.491   63   \n",
       "13         14      3.857         4.3929   20.086   67   \n",
       "14         15      4.055         3.3535   31.187   57   \n",
       "15         16      4.263         4.6646   21.328   66   \n",
       "16         17      4.349         0.6570   33.784   70   \n",
       "17         18      4.437         9.8749   38.475   66   \n",
       "18         19      4.759         0.5712   26.311   41   \n",
       "19         20      4.953         1.1972   46.063   70   \n",
       "20         21      5.155         3.1582   30.569   59   \n",
       "21         22      5.259         7.8460   33.115   60   \n",
       "22         23      5.474         0.5827   29.371   59   \n",
       "23         24      5.529         5.9299   31.500   63   \n",
       "24         25      5.641         1.4770   39.252   69   \n",
       "25         26      5.871         4.2631   22.646   68   \n",
       "26         27      6.050         1.6653   41.264   65   \n",
       "27         28      6.172         0.6703   47.942   67   \n",
       "28         29      6.360         2.8292   22.874   67   \n",
       "29         30      6.619        11.1340   29.371   65   \n",
       "..        ...        ...            ...      ...  ...   \n",
       "67         68     19.298         9.0250   57.397   72   \n",
       "68         69     19.298         0.6376   82.269   69   \n",
       "69         70     19.492         3.2871  119.104   72   \n",
       "70         71     20.287         6.4237   36.234   60   \n",
       "71         72     20.905         3.1899   28.219   77   \n",
       "72         73     21.328         3.3535   46.063   69   \n",
       "73         74     21.758         6.2965   25.534   60   \n",
       "74         75     26.576        20.0855   46.993   69   \n",
       "75         76     28.219        23.1039   26.050   68   \n",
       "76         77     29.666         7.4633   83.931   72   \n",
       "77         78     31.187        12.6797   77.478   78   \n",
       "78         79     31.817        14.1540   35.874   69   \n",
       "79         80     33.448        16.1190   45.604   63   \n",
       "80         81     33.784         4.3492   21.542   66   \n",
       "81         82     34.124        12.3049   32.137   57   \n",
       "82         83     35.517        13.5991   48.911   77   \n",
       "83         84     35.517        14.5851   46.525   65   \n",
       "84         85     36.234         4.7588   40.854   60   \n",
       "85         86     37.713        27.1126   33.784   64   \n",
       "86         87     39.646         7.5383   41.679   58   \n",
       "87         88     40.854         5.6407   29.079   62   \n",
       "88         89     53.517        16.6099  112.168   65   \n",
       "89         90     54.055         4.7588   40.447   76   \n",
       "90         91     56.261        25.7903   60.340   68   \n",
       "91         92     62.178        12.5535   39.646   61   \n",
       "92         93     80.640        16.9455   48.424   68   \n",
       "93         94    107.770        45.6042   49.402   44   \n",
       "94         95    170.716        18.3568   29.964   52   \n",
       "95         96    239.847        17.8143   43.380   68   \n",
       "96         97    265.072        32.1367   52.985   68   \n",
       "\n",
       "    benign_prostatic_hyperplasia  seminal_vesicle_invasion  \\\n",
       "0                         0.0000                         0   \n",
       "1                         0.0000                         0   \n",
       "2                         0.0000                         0   \n",
       "3                         0.0000                         0   \n",
       "4                         0.0000                         0   \n",
       "5                         0.0000                         0   \n",
       "6                         1.8589                         0   \n",
       "7                         4.6646                         0   \n",
       "8                         0.0000                         0   \n",
       "9                         0.0000                         0   \n",
       "10                        0.0000                         0   \n",
       "11                        3.5609                         0   \n",
       "12                        0.0000                         0   \n",
       "13                        0.0000                         0   \n",
       "14                        0.0000                         0   \n",
       "15                        0.0000                         0   \n",
       "16                        3.4556                         0   \n",
       "17                        0.0000                         0   \n",
       "18                        0.0000                         0   \n",
       "19                        5.2593                         0   \n",
       "20                        0.0000                         0   \n",
       "21                        4.3492                         0   \n",
       "22                        0.4493                         0   \n",
       "23                        1.5527                         0   \n",
       "24                        4.9530                         0   \n",
       "25                        1.3499                         0   \n",
       "26                        0.0000                         0   \n",
       "27                        6.1719                         0   \n",
       "28                        1.2461                         0   \n",
       "29                        0.0000                         0   \n",
       "..                           ...                       ...   \n",
       "67                       10.0744                         0   \n",
       "68                        0.0000                         0   \n",
       "69                       10.2779                         0   \n",
       "70                        0.0000                         1   \n",
       "71                        5.7546                         0   \n",
       "72                        0.0000                         1   \n",
       "73                        1.5527                         1   \n",
       "74                        0.0000                         1   \n",
       "75                        0.9512                         1   \n",
       "76                        8.3311                         0   \n",
       "77                       10.2779                         0   \n",
       "78                        0.0000                         1   \n",
       "79                        0.0000                         0   \n",
       "80                        1.7507                         0   \n",
       "81                        1.5527                         0   \n",
       "82                        0.5886                         1   \n",
       "83                        3.0649                         0   \n",
       "84                        5.4739                         0   \n",
       "85                        0.0000                         1   \n",
       "86                        5.1552                         0   \n",
       "87                        0.0000                         1   \n",
       "88                        0.0000                         1   \n",
       "89                        2.5600                         1   \n",
       "90                        0.0000                         0   \n",
       "91                        3.8574                         1   \n",
       "92                        0.0000                         1   \n",
       "93                        0.0000                         1   \n",
       "94                        0.0000                         1   \n",
       "95                        4.7588                         1   \n",
       "96                        1.5527                         1   \n",
       "\n",
       "    capsular_penetration  gleason_score  \n",
       "0                 0.0000              6  \n",
       "1                 0.0000              7  \n",
       "2                 0.0000              7  \n",
       "3                 0.0000              6  \n",
       "4                 0.0000              6  \n",
       "5                 0.0000              6  \n",
       "6                 0.0000              6  \n",
       "7                 0.0000              6  \n",
       "8                 0.0000              7  \n",
       "9                 0.0000              6  \n",
       "10                0.0000              6  \n",
       "11                0.0000              6  \n",
       "12                0.5488              7  \n",
       "13                0.0000              7  \n",
       "14                0.6505              7  \n",
       "15                0.0000              6  \n",
       "16                0.5488              7  \n",
       "17                1.4477              6  \n",
       "18                0.0000              6  \n",
       "19                0.0000              7  \n",
       "20                0.0000              6  \n",
       "21                3.8574              7  \n",
       "22                0.0000              6  \n",
       "23                3.2544              7  \n",
       "24                0.0000              6  \n",
       "25                0.0000              6  \n",
       "26                0.4493              7  \n",
       "27                0.0000              7  \n",
       "28                1.0513              7  \n",
       "29                5.0531              6  \n",
       "..                   ...            ...  \n",
       "67                0.6505              7  \n",
       "68                0.0000              6  \n",
       "69                0.4493              7  \n",
       "70                3.7434              7  \n",
       "71                0.0000              7  \n",
       "72                1.2461              7  \n",
       "73                3.2544              8  \n",
       "74                6.7531              8  \n",
       "75               11.2459              6  \n",
       "76                1.6487              8  \n",
       "77                0.0000              8  \n",
       "78               13.1971              7  \n",
       "79                1.4477              8  \n",
       "80                1.2461              7  \n",
       "81               10.2779              7  \n",
       "82                1.7507              7  \n",
       "83                5.7546              8  \n",
       "84                2.2479              8  \n",
       "85               10.2779              8  \n",
       "86                0.0000              6  \n",
       "87                1.3499              7  \n",
       "88               11.7048              8  \n",
       "89                2.2479              8  \n",
       "90                0.0000              6  \n",
       "91                0.0000              7  \n",
       "92                3.7434              8  \n",
       "93                8.7583              8  \n",
       "94               11.7048              8  \n",
       "95                4.7588              8  \n",
       "96               18.1741              8  \n",
       "\n",
       "[97 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['cancer_volume', 'weight', 'age', \n",
    "                'benign_prostatic_hyperplasia', 'seminal_vesicle_invasion', \n",
    "                'capsular_penetration', 'gleason_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ['psa_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_number</th>\n",
       "      <th>psa_level</th>\n",
       "      <th>cancer_volume</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>benign_prostatic_hyperplasia</th>\n",
       "      <th>seminal_vesicle_invasion</th>\n",
       "      <th>capsular_penetration</th>\n",
       "      <th>gleason_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.5599</td>\n",
       "      <td>15.959</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.3716</td>\n",
       "      <td>27.660</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.6005</td>\n",
       "      <td>14.732</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>26.576</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.448</td>\n",
       "      <td>2.1170</td>\n",
       "      <td>30.877</td>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_number  psa_level  cancer_volume  weight  age  \\\n",
       "0          1      0.651         0.5599  15.959   50   \n",
       "1          2      0.852         0.3716  27.660   58   \n",
       "2          3      0.852         0.6005  14.732   74   \n",
       "3          4      0.852         0.3012  26.576   58   \n",
       "4          5      1.448         2.1170  30.877   62   \n",
       "\n",
       "   benign_prostatic_hyperplasia  seminal_vesicle_invasion  \\\n",
       "0                           0.0                         0   \n",
       "1                           0.0                         0   \n",
       "2                           0.0                         0   \n",
       "3                           0.0                         0   \n",
       "4                           0.0                         0   \n",
       "\n",
       "   capsular_penetration  gleason_score  \n",
       "0                   0.0              6  \n",
       "1                   0.0              7  \n",
       "2                   0.0              7  \n",
       "3                   0.0              6  \n",
       "4                   0.0              6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = data[cat_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = data[num_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X_cat.values\n",
    "X_num = X_num.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tst = np.array([4.2633, 22.783, 68, 1.3500, 0, 0, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.2633,  22.783 ,  68.    ,   1.35  ,   0.    ,   0.    ])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.564</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   18.96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 15 Apr 2018</td> <th>  Prob (F-statistic):</th> <td>2.50e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:19:39</td>     <th>  Log-Likelihood:    </th> <td> -467.16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    97</td>      <th>  AIC:               </th> <td>   948.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    90</td>      <th>  BIC:               </th> <td>   966.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    2.0549</td> <td>    0.588</td> <td>    3.496</td> <td> 0.001</td> <td>    0.887</td> <td>    3.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    0.0091</td> <td>    0.073</td> <td>    0.124</td> <td> 0.902</td> <td>   -0.137</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>   -0.6362</td> <td>    0.394</td> <td>   -1.613</td> <td> 0.110</td> <td>   -1.420</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>    1.4053</td> <td>    1.162</td> <td>    1.209</td> <td> 0.230</td> <td>   -0.903</td> <td>    3.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>   20.2485</td> <td>   10.707</td> <td>    1.891</td> <td> 0.062</td> <td>   -1.023</td> <td>   41.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th> <td>    1.1540</td> <td>    1.319</td> <td>    0.875</td> <td> 0.384</td> <td>   -1.467</td> <td>    3.775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th> <td>    5.6900</td> <td>    3.687</td> <td>    1.543</td> <td> 0.126</td> <td>   -1.634</td> <td>   13.014</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>95.209</td> <th>  Durbin-Watson:     </th> <td>   0.822</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>1166.316</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 3.126</td> <th>  Prob(JB):          </th> <td>5.47e-254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>18.795</td> <th>  Cond. No.          </th> <td>    289.</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.596\n",
       "Model:                            OLS   Adj. R-squared:                  0.564\n",
       "Method:                 Least Squares   F-statistic:                     18.96\n",
       "Date:                Sun, 15 Apr 2018   Prob (F-statistic):           2.50e-15\n",
       "Time:                        23:19:39   Log-Likelihood:                -467.16\n",
       "No. Observations:                  97   AIC:                             948.3\n",
       "Df Residuals:                      90   BIC:                             966.3\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             2.0549      0.588      3.496      0.001       0.887       3.222\n",
       "x2             0.0091      0.073      0.124      0.902      -0.137       0.155\n",
       "x3            -0.6362      0.394     -1.613      0.110      -1.420       0.147\n",
       "x4             1.4053      1.162      1.209      0.230      -0.903       3.714\n",
       "x5            20.2485     10.707      1.891      0.062      -1.023      41.520\n",
       "x6             1.1540      1.319      0.875      0.384      -1.467       3.775\n",
       "x7             5.6900      3.687      1.543      0.126      -1.634      13.014\n",
       "==============================================================================\n",
       "Omnibus:                       95.209   Durbin-Watson:                   0.822\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1166.316\n",
       "Skew:                           3.126   Prob(JB):                    5.47e-254\n",
       "Kurtosis:                      18.795   Cond. No.                         289.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = result.get_prediction(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean_se</th>\n",
       "      <th>mean_ci_lower</th>\n",
       "      <th>mean_ci_upper</th>\n",
       "      <th>obs_ci_lower</th>\n",
       "      <th>obs_ci_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.739799</td>\n",
       "      <td>6.264748</td>\n",
       "      <td>-10.706216</td>\n",
       "      <td>14.185815</td>\n",
       "      <td>-61.128654</td>\n",
       "      <td>64.608252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean   mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n",
       "0  1.739799  6.264748     -10.706216      14.185815    -61.128654   \n",
       "\n",
       "   obs_ci_upper  \n",
       "0     64.608252  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.summary_frame(alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result is bad (Only 0.564 adjusted R-Square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try log(y) as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.651,    0.852,    0.852,    0.852,    1.448,    2.16 ,\n",
       "          2.16 ,    2.34 ,    2.858,    2.858,    3.561,    3.561,\n",
       "          3.561,    3.857,    4.055,    4.263,    4.349,    4.437,\n",
       "          4.759,    4.953,    5.155,    5.259,    5.474,    5.529,\n",
       "          5.641,    5.871,    6.05 ,    6.172,    6.36 ,    6.619,\n",
       "          6.821,    7.463,    7.463,    7.538,    7.768,    8.085,\n",
       "          8.671,    8.935,    9.116,    9.777,    9.974,   10.074,\n",
       "         10.278,   10.697,   12.429,   12.807,   13.066,   13.066,\n",
       "         13.33 ,   13.33 ,   14.296,   14.585,   14.585,   14.732,\n",
       "         14.88 ,   15.18 ,   16.281,   16.281,   16.61 ,   16.61 ,\n",
       "         17.116,   17.288,   17.288,   17.814,   17.814,   17.993,\n",
       "         18.541,   19.298,   19.298,   19.492,   20.287,   20.905,\n",
       "         21.328,   21.758,   26.576,   28.219,   29.666,   31.187,\n",
       "         31.817,   33.448,   33.784,   34.124,   35.517,   35.517,\n",
       "         36.234,   37.713,   39.646,   40.854,   53.517,   54.055,\n",
       "         56.261,   62.178,   80.64 ,  107.77 ,  170.716,  239.847,  265.072])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = [math.log(i) for i in y[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm.OLS(y_log,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   18.24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 17 Apr 2018</td> <th>  Prob (F-statistic):</th> <td>7.69e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:57:59</td>     <th>  Log-Likelihood:    </th> <td> -107.84</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    97</td>      <th>  AIC:               </th> <td>   231.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    89</td>      <th>  BIC:               </th> <td>   252.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -0.6858</td> <td>    0.999</td> <td>   -0.687</td> <td> 0.494</td> <td>   -2.670</td> <td>    1.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0695</td> <td>    0.015</td> <td>    4.749</td> <td> 0.000</td> <td>    0.040</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0014</td> <td>    0.002</td> <td>    0.757</td> <td> 0.451</td> <td>   -0.002</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0028</td> <td>    0.012</td> <td>   -0.239</td> <td> 0.812</td> <td>   -0.026</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0875</td> <td>    0.030</td> <td>    2.955</td> <td> 0.004</td> <td>    0.029</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.7826</td> <td>    0.268</td> <td>    2.917</td> <td> 0.004</td> <td>    0.249</td> <td>    1.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.0265</td> <td>    0.033</td> <td>   -0.807</td> <td> 0.422</td> <td>   -0.092</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.3582</td> <td>    0.128</td> <td>    2.799</td> <td> 0.006</td> <td>    0.104</td> <td>    0.612</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.096</td> <th>  Durbin-Watson:     </th> <td>   1.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.213</td> <th>  Jarque-Bera (JB):  </th> <td>   2.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.418</td> <th>  Prob(JB):          </th> <td>   0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.991</td> <th>  Cond. No.          </th> <td>1.09e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.589\n",
       "Model:                            OLS   Adj. R-squared:                  0.557\n",
       "Method:                 Least Squares   F-statistic:                     18.24\n",
       "Date:                Tue, 17 Apr 2018   Prob (F-statistic):           7.69e-15\n",
       "Time:                        19:57:59   Log-Likelihood:                -107.84\n",
       "No. Observations:                  97   AIC:                             231.7\n",
       "Df Residuals:                      89   BIC:                             252.3\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.6858      0.999     -0.687      0.494      -2.670       1.299\n",
       "x1             0.0695      0.015      4.749      0.000       0.040       0.099\n",
       "x2             0.0014      0.002      0.757      0.451      -0.002       0.005\n",
       "x3            -0.0028      0.012     -0.239      0.812      -0.026       0.020\n",
       "x4             0.0875      0.030      2.955      0.004       0.029       0.146\n",
       "x5             0.7826      0.268      2.917      0.004       0.249       1.316\n",
       "x6            -0.0265      0.033     -0.807      0.422      -0.092       0.039\n",
       "x7             0.3582      0.128      2.799      0.006       0.104       0.612\n",
       "==============================================================================\n",
       "Omnibus:                        3.096   Durbin-Watson:                   1.085\n",
       "Prob(Omnibus):                  0.213   Jarque-Bera (JB):                2.821\n",
       "Skew:                          -0.418   Prob(JB):                        0.244\n",
       "Kurtosis:                       2.991   Cond. No.                     1.09e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.09e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -8.17014320e-01,  -6.46150891e-01,\n",
       "         -1.86242597e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -8.40907626e-01,  -3.90139812e-01,\n",
       "         -7.87896192e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -8.11862604e-01,  -6.72996938e-01,\n",
       "          1.36116337e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -8.49840651e-01,  -4.13857102e-01,\n",
       "         -7.87896192e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -6.19434576e-01,  -3.19753721e-01,\n",
       "         -2.50631302e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -8.43661129e-01,  -4.42212829e-01,\n",
       "         -1.86242597e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -6.22111945e-01,  -2.92185654e-01,\n",
       "          1.80011432e-02,  -2.22957959e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -6.35080059e-01,  -2.41206608e-01,\n",
       "         -7.87896192e-01,   7.02656490e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -8.29893611e-01,  -2.41206608e-01,\n",
       "         -2.26537464e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -7.29942699e-01,  -4.36655456e-01,\n",
       "         -1.16315079e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -7.25133585e-01,  -1.94581566e-01,\n",
       "          1.52317366e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -8.55170013e-01,  -1.94581566e-01,\n",
       "         -1.16315079e-01,   3.38540345e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -2.53256663e-01,  -5.46993365e-01,\n",
       "         -1.16315079e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -4.48432340e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -3.30646610e-01,  -5.55854530e-01,\n",
       "          4.20949811e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -4.62535626e-01,  -3.12971102e-01,\n",
       "         -9.22212415e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -4.21551251e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -2.96170715e-01,  -5.28680292e-01,\n",
       "          2.86633588e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -8.04693343e-01,  -2.56150251e-01,\n",
       "          8.23898479e-01,   3.03801349e-01,   0.00000000e+00,\n",
       "         -4.48432340e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   3.64961997e-01,  -1.53513897e-01,\n",
       "          2.86633588e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -2.10837345e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -8.15580468e-01,  -4.19655148e-01,\n",
       "         -3.07127198e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -7.36147599e-01,   1.25071329e-02,\n",
       "          8.23898479e-01,   8.98850992e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -4.87317158e-01,  -3.26492582e-01,\n",
       "         -6.53579970e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,   1.07515754e-01,  -2.70787582e-01,\n",
       "         -5.19263747e-01,   5.98604454e-01,   0.00000000e+00,\n",
       "          4.26088520e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -8.14121238e-01,  -3.52704126e-01,\n",
       "         -6.53579970e-01,  -6.87992043e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -1.35617345e-01,  -3.06122843e-01,\n",
       "         -1.16315079e-01,  -3.23974870e-01,   0.00000000e+00,\n",
       "          2.66705071e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -7.00643898e-01,  -1.36513589e-01,\n",
       "          6.89582256e-01,   7.97801091e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -3.47116876e-01,  -4.99843218e-01,\n",
       "          5.55266033e-01,  -3.90879603e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -6.76750593e-01,  -9.24921980e-02,\n",
       "          1.52317366e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -4.74731931e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -8.03005712e-01,   5.36185609e-02,\n",
       "          4.20949811e-01,   1.19992229e+00,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -5.29063826e-01,  -4.94854710e-01,\n",
       "          4.20949811e-01,  -4.25123742e-01,   0.00000000e+00,\n",
       "         -3.15612799e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   5.24728650e-01,  -3.52704126e-01,\n",
       "          1.52317366e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "          7.42132947e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -7.18484572e-01,   3.11751944e-01,\n",
       "          1.52317366e-01,   1.50587619e+00,   0.00000000e+00,\n",
       "         -4.74731931e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -7.36147599e-01,   8.85783106e+00,\n",
       "          1.52317366e-01,   9.69648604e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -4.31688772e-01,  -5.37935286e-01,\n",
       "          9.58214701e-01,   3.38540345e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -7.59888637e-01,  -4.19655148e-01,\n",
       "         -1.32516108e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -7.62439118e-01,  -4.47726442e-01,\n",
       "         -1.16315079e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -4.74731931e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -4.17781676e-01,   3.51550607e-01,\n",
       "          1.80011432e-02,   2.05318858e+00,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -3.63104961e-01,  -1.45068441e-01,\n",
       "          1.22684715e+00,  -6.51504550e-01,   0.00000000e+00,\n",
       "          7.96635214e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -6.87054026e-01,  -7.61280393e-01,\n",
       "          1.80011432e-02,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   9.25991528e-01,   3.11751944e-01,\n",
       "          5.55266033e-01,   4.62089767e-01,   1.00000000e+00,\n",
       "          1.05423904e+00,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -6.05667058e-01,  -5.51456767e-01,\n",
       "         -1.05652864e+00,   8.33843382e-03,   0.00000000e+00,\n",
       "         -3.68264844e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -6.52184773e-01,  -4.89822444e-01,\n",
       "         -5.19263747e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -3.52496993e-01,  -1.27893098e-01,\n",
       "          5.55266033e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -6.61435022e-01,   5.36185609e-02,\n",
       "         -2.50631302e-01,   9.87826370e-01,   0.00000000e+00,\n",
       "         -4.21551251e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -1.43103830e-01,   8.55625124e-02,\n",
       "         -3.84947524e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "          6.69513487e-04,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -3.25038091e-01,  -3.33143926e-01,\n",
       "          2.86633588e-01,   1.06225294e+00,   0.00000000e+00,\n",
       "         -4.21551251e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -2.20709489e-01,  -3.46249698e-01,\n",
       "         -3.84947524e-01,  -2.22957959e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   1.05752525e+00,   1.99247973e-01,\n",
       "          2.03274448e+00,   1.32581404e+00,   1.00000000e+00,\n",
       "          3.18527240e+00,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -4.83294759e-01,   2.47995318e-01,\n",
       "          5.55266033e-01,   9.87826370e-01,   0.00000000e+00,\n",
       "         -4.21551251e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -1.57861087e-01,  -2.70787582e-01,\n",
       "         -2.80263953e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -4.58259447e-01,  -2.18233218e-01,\n",
       "          8.23898479e-01,   4.62089767e-01,   0.00000000e+00,\n",
       "         -4.74731931e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -5.10652150e-01,   1.87367448e-01,\n",
       "          5.55266033e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -2.20709489e-01,   5.08163487e-01,\n",
       "          1.80011432e-02,   1.77821276e+00,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -6.76750593e-01,  -1.70186014e-01,\n",
       "          1.80011432e-02,   6.27603103e-01,   0.00000000e+00,\n",
       "         -3.15612799e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   1.79703229e-01,   3.51550607e-01,\n",
       "          5.55266033e-01,   1.10062089e+00,   0.00000000e+00,\n",
       "          5.33322127e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   2.07304784e+00,  -2.56150251e-01,\n",
       "         -6.53579970e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -4.36218730e-01,   5.85244680e-01,\n",
       "          2.86633588e-01,   1.91225316e+00,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -5.53337800e-01,  -6.09437227e-01,\n",
       "         -2.26537464e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -1.57709529e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -6.87054026e-01,  -5.56472567e-02,\n",
       "         -1.99674220e+00,   5.28631604e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -6.70317292e-01,   4.34845555e-01,\n",
       "          8.23898479e-01,  -3.23974870e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -5.21805743e-01,   3.28549924e-02,\n",
       "         -3.84947524e-01,   3.62260514e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -6.87054026e-01,   1.01399377e+00,\n",
       "          1.22684715e+00,   2.55451221e+00,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,   4.95399108e-02,  -9.24921980e-02,\n",
       "         -1.16315079e-01,   8.30824582e-01,   1.00000000e+00,\n",
       "          1.19147269e+00,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   1.15727314e+00,  -2.56150251e-01,\n",
       "          1.09253092e+00,  -8.36218358e-01,   0.00000000e+00,\n",
       "          6.64344308e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   7.80900634e-02,   1.07398172e-01,\n",
       "          2.86633588e-01,   1.62596161e+00,   1.00000000e+00,\n",
       "          1.58665377e+00,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   1.17514652e-01,  -1.78390796e-01,\n",
       "          1.80011432e-02,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -3.41673313e-01,   2.26154244e-02,\n",
       "         -3.84947524e-01,   3.98748007e-01,   0.00000000e+00,\n",
       "         -4.21551251e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   6.84718342e-02,   6.41644408e-02,\n",
       "          5.55266033e-01,   1.12008528e+00,   0.00000000e+00,\n",
       "          3.95956325e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   2.57118554e-01,   2.60488466e-01,\n",
       "          1.09253092e+00,   2.48737654e+00,   0.00000000e+00,\n",
       "         -4.21551251e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -8.07155001e-01,   8.04673371e-01,\n",
       "          6.89582256e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -4.70961093e-01,   1.61060176e+00,\n",
       "          1.09253092e+00,   2.55451221e+00,   0.00000000e+00,\n",
       "         -4.74731931e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -7.29592772e-02,  -2.02545675e-01,\n",
       "         -5.19263747e-01,  -8.36218358e-01,   1.00000000e+00,\n",
       "          3.95956325e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -4.83294759e-01,  -3.77909217e-01,\n",
       "          1.76411204e+00,   1.06225294e+00,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -4.62535626e-01,   1.25071329e-02,\n",
       "          6.89582256e-01,  -8.36218358e-01,   1.00000000e+00,\n",
       "         -2.64123752e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,  -8.90996301e-02,  -4.36655456e-01,\n",
       "         -5.19263747e-01,  -3.23974870e-01,   1.00000000e+00,\n",
       "          2.66705071e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   1.66058061e+00,   3.28549924e-02,\n",
       "          6.89582256e-01,  -8.36218358e-01,   1.00000000e+00,\n",
       "          1.19147269e+00,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   2.04358408e+00,  -4.25365676e-01,\n",
       "          5.55266033e-01,  -5.22412725e-01,   1.00000000e+00,\n",
       "          2.37899832e+00,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,   5.89551167e-02,   8.41036965e-01,\n",
       "          1.09253092e+00,   1.91225316e+00,   0.00000000e+00,\n",
       "         -1.57709529e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   7.20861854e-01,   6.99849075e-01,\n",
       "          1.89842826e+00,   2.55451221e+00,   0.00000000e+00,\n",
       "         -5.93489781e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   9.07935143e-01,  -2.10422265e-01,\n",
       "          6.89582256e-01,  -8.36218358e-01,   1.00000000e+00,\n",
       "          2.89473462e+00,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   1.15727314e+00,   2.46447969e-03,\n",
       "         -1.16315079e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -2.10837345e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -3.36191684e-01,  -5.23998096e-01,\n",
       "          2.86633588e-01,  -2.58653680e-01,   0.00000000e+00,\n",
       "         -2.64123752e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   6.73303644e-01,  -2.92185654e-01,\n",
       "         -9.22212415e-01,  -3.23974870e-01,   0.00000000e+00,\n",
       "          2.12313899e+00,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   8.37524122e-01,   7.48197177e-02,\n",
       "          1.76411204e+00,  -6.42036277e-01,   1.00000000e+00,\n",
       "         -1.30749144e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   9.62637235e-01,   2.26154244e-02,\n",
       "          1.52317366e-01,   1.74907467e-01,   0.00000000e+00,\n",
       "          9.27551669e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -2.84217717e-01,  -1.01462760e-01,\n",
       "         -5.19263747e-01,   9.69648604e-01,   0.00000000e+00,\n",
       "          6.69513487e-04,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   2.55224629e+00,  -2.56150251e-01,\n",
       "          1.80011432e-02,  -8.36218358e-01,   1.00000000e+00,\n",
       "          2.12313899e+00,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   6.84718342e-02,  -8.34122392e-02,\n",
       "         -7.87896192e-01,   8.64507882e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,  -1.72313808e-01,  -3.59092916e-01,\n",
       "         -2.50631302e-01,  -8.36218358e-01,   1.00000000e+00,\n",
       "         -2.36687596e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   1.21956323e+00,   1.45884611e+00,\n",
       "          1.52317366e-01,  -8.36218358e-01,   1.00000000e+00,\n",
       "          2.50029362e+00,   1.51931133e+00],\n",
       "       [  1.00000000e+00,  -2.84217717e-01,  -1.10367683e-01,\n",
       "          1.62979581e+00,   8.33843382e-03,   1.00000000e+00,\n",
       "          6.69513487e-04,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   2.38446021e+00,   3.24879595e-01,\n",
       "          5.55266033e-01,  -8.36218358e-01,   0.00000000e+00,\n",
       "         -5.93489781e-01,  -1.18478407e+00],\n",
       "       [  1.00000000e+00,   7.04848391e-01,  -1.27893098e-01,\n",
       "         -3.84947524e-01,   4.36357177e-01,   1.00000000e+00,\n",
       "         -5.93489781e-01,   1.67263633e-01],\n",
       "       [  1.00000000e+00,   1.26214737e+00,   6.41644408e-02,\n",
       "          5.55266033e-01,  -8.36218358e-01,   1.00000000e+00,\n",
       "          3.95956325e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   4.89863740e+00,   8.55625124e-02,\n",
       "         -2.66832331e+00,  -8.36218358e-01,   1.00000000e+00,\n",
       "          1.72148212e+00,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   1.44122662e+00,  -3.39729631e-01,\n",
       "         -1.59379353e+00,  -8.36218358e-01,   1.00000000e+00,\n",
       "          2.50029362e+00,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   1.37238902e+00,  -4.61953478e-02,\n",
       "          5.55266033e-01,   7.33733541e-01,   1.00000000e+00,\n",
       "          6.64344308e-01,   1.51931133e+00],\n",
       "       [  1.00000000e+00,   3.18975216e+00,   1.63956470e-01,\n",
       "          5.55266033e-01,  -3.23974870e-01,   1.00000000e+00,\n",
       "          4.21024278e+00,   1.51931133e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6094379124341003"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.999999999999999"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(math.log(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try standardization for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaler_y = preprocessing.StandardScaler().fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.        ,   6.99868247,  45.49136082,  63.86597938,\n",
       "         2.53472474,   0.21649485,   2.24536701,   6.87628866])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,  -1.37347178e-17,   2.06020767e-16,\n",
       "         4.13186095e-16,  -4.57823928e-17,  -3.66259142e-17,\n",
       "        -4.57823928e-17,  -6.36375259e-16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaled = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_after_rescale = sm.OLS(y,X[:,[1,2,3,6]]).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.59590000e+01,   5.00000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.76600000e+01,   5.80000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  1.47320000e+01,   7.40000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.65760000e+01,   5.80000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.08770000e+01,   6.20000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.52800000e+01,   5.00000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.21370000e+01,   6.40000000e+01,   1.85890000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.44670000e+01,   5.80000000e+01,   4.66460000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.44670000e+01,   4.70000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.55340000e+01,   6.30000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.65980000e+01,   6.50000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.65980000e+01,   6.30000000e+01,   3.56090000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.04910000e+01,   6.30000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.00860000e+01,   6.70000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.11870000e+01,   5.70000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.13280000e+01,   6.60000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.37840000e+01,   7.00000000e+01,   3.45560000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.84750000e+01,   6.60000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.63110000e+01,   4.10000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  4.60630000e+01,   7.00000000e+01,   5.25930000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.05690000e+01,   5.90000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.31150000e+01,   6.00000000e+01,   4.34920000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.93710000e+01,   5.90000000e+01,   4.49300000e-01,\n",
       "          6.00000000e+00],\n",
       "       [  3.15000000e+01,   6.30000000e+01,   1.55270000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.92520000e+01,   6.90000000e+01,   4.95300000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.26460000e+01,   6.80000000e+01,   1.34990000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  4.12640000e+01,   6.50000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.79420000e+01,   6.70000000e+01,   6.17190000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.28740000e+01,   6.70000000e+01,   1.24610000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.93710000e+01,   6.50000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  5.97400000e+01,   6.50000000e+01,   7.09930000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  4.50339000e+02,   6.50000000e+01,   5.47390000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.09050000e+01,   7.10000000e+01,   3.56090000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.63110000e+01,   5.40000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.50280000e+01,   6.30000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  6.15590000e+01,   6.40000000e+01,   8.75830000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.88610000e+01,   7.30000000e+01,   5.59900000e-01,\n",
       "          8.00000000e+00],\n",
       "       [  1.06970000e+01,   6.40000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  5.97400000e+01,   6.80000000e+01,   3.93540000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.02870000e+01,   5.60000000e+01,   2.56000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.31040000e+01,   6.00000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  3.96460000e+01,   6.80000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.79420000e+01,   6.20000000e+01,   5.52900000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  4.94020000e+01,   6.10000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.02650000e+01,   6.60000000e+01,   5.75460000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.96660000e+01,   6.10000000e+01,   1.85890000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  5.45980000e+01,   7.90000000e+01,   6.55350000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  5.68260000e+01,   6.80000000e+01,   5.52900000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.31150000e+01,   4.30000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.55170000e+01,   7.00000000e+01,   3.93540000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  5.40550000e+01,   6.80000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  6.87170000e+01,   6.40000000e+01,   7.92480000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.77130000e+01,   6.40000000e+01,   4.43710000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  6.15590000e+01,   6.80000000e+01,   5.87090000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.37840000e+01,   5.90000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  7.22400000e+01,   6.60000000e+01,   8.33110000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  1.76370000e+01,   4.70000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.29480000e+01,   4.90000000e+01,   4.13710000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  6.53660000e+01,   7.00000000e+01,   1.55270000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  4.69930000e+01,   6.10000000e+01,   3.63280000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  9.18360000e+01,   7.30000000e+01,   1.02779000e+01,\n",
       "          6.00000000e+00],\n",
       "       [  4.12640000e+01,   6.30000000e+01,   5.05310000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.37840000e+01,   7.20000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  5.04000000e+01,   6.60000000e+01,   7.46330000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.73380000e+01,   6.40000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  4.65250000e+01,   6.10000000e+01,   3.74340000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.84240000e+01,   6.80000000e+01,   5.92990000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  5.73970000e+01,   7.20000000e+01,   1.00744000e+01,\n",
       "          7.00000000e+00],\n",
       "       [  8.22690000e+01,   6.90000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  1.19104000e+02,   7.20000000e+01,   1.02779000e+01,\n",
       "          7.00000000e+00],\n",
       "       [  3.62340000e+01,   6.00000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.82190000e+01,   7.70000000e+01,   5.75460000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.60630000e+01,   6.90000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  2.55340000e+01,   6.00000000e+01,   1.55270000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  4.69930000e+01,   6.90000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  2.60500000e+01,   6.80000000e+01,   9.51200000e-01,\n",
       "          6.00000000e+00],\n",
       "       [  8.39310000e+01,   7.20000000e+01,   8.33110000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  7.74780000e+01,   7.80000000e+01,   1.02779000e+01,\n",
       "          8.00000000e+00],\n",
       "       [  3.58740000e+01,   6.90000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.56040000e+01,   6.30000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  2.15420000e+01,   6.60000000e+01,   1.75070000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  3.21370000e+01,   5.70000000e+01,   1.55270000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.89110000e+01,   7.70000000e+01,   5.88600000e-01,\n",
       "          7.00000000e+00],\n",
       "       [  4.65250000e+01,   6.50000000e+01,   3.06490000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  4.08540000e+01,   6.00000000e+01,   5.47390000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  3.37840000e+01,   6.40000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  4.16790000e+01,   5.80000000e+01,   5.15520000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  2.90790000e+01,   6.20000000e+01,   0.00000000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  1.12168000e+02,   6.50000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  4.04470000e+01,   7.60000000e+01,   2.56000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  6.03400000e+01,   6.80000000e+01,   0.00000000e+00,\n",
       "          6.00000000e+00],\n",
       "       [  3.96460000e+01,   6.10000000e+01,   3.85740000e+00,\n",
       "          7.00000000e+00],\n",
       "       [  4.84240000e+01,   6.80000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  4.94020000e+01,   4.40000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  2.99640000e+01,   5.20000000e+01,   0.00000000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  4.33800000e+01,   6.80000000e+01,   4.75880000e+00,\n",
       "          8.00000000e+00],\n",
       "       [  5.29850000e+01,   6.80000000e+01,   1.55270000e+00,\n",
       "          8.00000000e+00]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,[1,2,3,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   12.72</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 10 Apr 2018</td> <th>  Prob (F-statistic):</th> <td>2.70e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:45:40</td>     <th>  Log-Likelihood:    </th> <td> -489.95</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    97</td>      <th>  AIC:               </th> <td>   987.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    93</td>      <th>  BIC:               </th> <td>   998.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.0370</td> <td>    0.091</td> <td>    0.406</td> <td> 0.686</td> <td>   -0.144</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>   -1.3729</td> <td>    0.473</td> <td>   -2.900</td> <td> 0.005</td> <td>   -2.313</td> <td>   -0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    0.4905</td> <td>    1.434</td> <td>    0.342</td> <td> 0.733</td> <td>   -2.358</td> <td>    3.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>   15.9017</td> <td>    4.225</td> <td>    3.764</td> <td> 0.000</td> <td>    7.512</td> <td>   24.291</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>117.862</td> <th>  Durbin-Watson:     </th> <td>   0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2000.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 4.181</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>23.619</td>  <th>  Cond. No.          </th> <td>    91.9</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.354\n",
       "Model:                            OLS   Adj. R-squared:                  0.326\n",
       "Method:                 Least Squares   F-statistic:                     12.72\n",
       "Date:                Tue, 10 Apr 2018   Prob (F-statistic):           2.70e-08\n",
       "Time:                        00:45:40   Log-Likelihood:                -489.95\n",
       "No. Observations:                  97   AIC:                             987.9\n",
       "Df Residuals:                      93   BIC:                             998.2\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0370      0.091      0.406      0.686      -0.144       0.218\n",
       "x2            -1.3729      0.473     -2.900      0.005      -2.313      -0.433\n",
       "x3             0.4905      1.434      0.342      0.733      -2.358       3.339\n",
       "x4            15.9017      4.225      3.764      0.000       7.512      24.291\n",
       "==============================================================================\n",
       "Omnibus:                      117.862   Durbin-Watson:                   0.297\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2000.898\n",
       "Skew:                           4.181   Prob(JB):                         0.00\n",
       "Kurtosis:                      23.619   Cond. No.                         91.9\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_after_rescale.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-b9cedfb2a27d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_after_rescale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsquared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "result_after_rescale.rsquared()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVR(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(svm_clf, X, y_log, cv=10, scoring = scoring, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 0.19852424,  0.27666783,  0.13123393,  0.0447011 ,  0.21300673,\n",
       "         0.16090918,  0.23928881,  0.14314771,  0.14613509,  0.128824  ]),\n",
       " 'score_time': array([ 0.00078678,  0.00050306,  0.00039005,  0.00039792,  0.00039506,\n",
       "         0.00117397,  0.000422  ,  0.0003531 ,  0.00038505,  0.00035501]),\n",
       " 'test_r2': array([  -8.27380652,  -32.45164232,  -31.91460393, -352.35440854,\n",
       "         -35.93352021, -180.35005012, -259.42996213,   -9.01793607,\n",
       "        -187.27172035,   -2.43643959])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestRegressor(oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_rf = RFECV(estimator = rf_clf, step = 1, cv = 3, scoring = 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/ensemble/forest.py:720: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFECV(cv=3,\n",
       "   estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=True, random_state=None, verbose=0, warm_start=False),\n",
       "   n_jobs=1, scoring='r2', step=1, verbose=0)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22.43530899, -20.59137335, -18.08458691, -23.78854811,\n",
       "       -18.32294291, -13.4050702 , -19.42198998])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_rf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'age', True),\n",
       " (1, 'benign_prostatic_hyperplasia', True),\n",
       " (1, 'cancer_volume', True),\n",
       " (1, 'capsular_penetration', True),\n",
       " (1, 'seminal_vesicle_invasion', True),\n",
       " (1, 'weight', True),\n",
       " (2, 'gleason_score', False)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rfe_rf.ranking_, predictors, rfe_rf.support_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17124294247799488"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_rf.estimator_.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate, RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\"n_estimators\": [25, 50, 100, 200, 400, 800, 1600],\n",
    "                    \"max_features\": [\"sqrt\", \"log2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(RandomForestRegressor(), tuned_parameters, cv=3, scoring='r2', n_jobs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_search.py:725: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'n_estimators': [25, 50, 100, 200, 400, 800, 1600], 'max_features': ['sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 'sqrt', 'n_estimators': 400}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestRegressor(n_estimators=400, max_features = 'sqrt', oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n",
       "           oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28222511625337965"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90472574489163338"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gleason_score'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictors.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cancer_volume',\n",
       " 'weight',\n",
       " 'age',\n",
       " 'benign_prostatic_hyperplasia',\n",
       " 'seminal_vesicle_invasion',\n",
       " 'capsular_penetration']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = rf_clf.predict(X)\n",
    "SS_Residual = sum((y[:,0]-yhat)**2)\n",
    "SS_Total = sum((y[:,0]-np.mean(y[:,0]))**2)\n",
    "r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89837412788440907"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.651,    0.852,    0.852,    0.852,    1.448,    2.16 ,\n",
       "          2.16 ,    2.34 ,    2.858,    2.858,    3.561,    3.561,\n",
       "          3.561,    3.857,    4.055,    4.263,    4.349,    4.437,\n",
       "          4.759,    4.953,    5.155,    5.259,    5.474,    5.529,\n",
       "          5.641,    5.871,    6.05 ,    6.172,    6.36 ,    6.619,\n",
       "          6.821,    7.463,    7.463,    7.538,    7.768,    8.085,\n",
       "          8.671,    8.935,    9.116,    9.777,    9.974,   10.074,\n",
       "         10.278,   10.697,   12.429,   12.807,   13.066,   13.066,\n",
       "         13.33 ,   13.33 ,   14.296,   14.585,   14.585,   14.732,\n",
       "         14.88 ,   15.18 ,   16.281,   16.281,   16.61 ,   16.61 ,\n",
       "         17.116,   17.288,   17.288,   17.814,   17.814,   17.993,\n",
       "         18.541,   19.298,   19.298,   19.492,   20.287,   20.905,\n",
       "         21.328,   21.758,   26.576,   28.219,   29.666,   31.187,\n",
       "         31.817,   33.448,   33.784,   34.124,   35.517,   35.517,\n",
       "         36.234,   37.713,   39.646,   40.854,   53.517,   54.055,\n",
       "         56.261,   62.178,   80.64 ,  107.77 ,  170.716,  239.847,  265.072])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.090514484543693194, 'age'),\n",
       " (0.094326441472253234, 'benign_prostatic_hyperplasia'),\n",
       " (0.098401402598290139, 'weight'),\n",
       " (0.099312601177703269, 'seminal_vesicle_invasion'),\n",
       " (0.24979840953311686, 'capsular_penetration'),\n",
       " (0.36764666067494312, 'cancer_volume')]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf_clf.feature_importances_,predictors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI = sorted(zip(map(lambda x: round(x, 4), rf_clf.feature_importances_), predictors), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI_arr = np.array(FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = FI_arr[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = FI_arr[:,0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Predictor Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.figure(figsize=(40,40))\n",
    "fig = plt.figure(figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAEWCAYAAACOk1WwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1533b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c15420cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c15420b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()\n",
    "fig.savefig('temp.png', dpi=fig.dpi)\n",
    "fig = plt.figure(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_ints(model, X, percentile=95):\n",
    "    err_down = []\n",
    "    err_up = []\n",
    "#    for x in range(len(X)):\n",
    "    preds = []\n",
    "    for pred in model.estimators_:\n",
    "        preds.append(pred.predict(X)[0])\n",
    "    err_down.append(np.percentile(preds, (100 - percentile) / 2. ))\n",
    "    err_up.append(np.percentile(preds, 100 - (100 - percentile) / 2.))\n",
    "    return err_down, err_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.2633,  22.783 ,  68.    ,   1.35  ,   0.    ,   0.    ]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tst=x_tst[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tst=x_tst.reshape(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.2633],\n",
       "       [ 22.783 ],\n",
       "       [ 68.    ],\n",
       "       [  1.35  ],\n",
       "       [  0.    ],\n",
       "       [  0.    ]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.857])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.estimators_[0].predict(x_tst[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.8569999999999998], [14.295999999999999])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ints(rf_clf,x_tst,90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_clf = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/xinsongdu/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(gbdt_clf, X, y, cv=2, scoring = scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 0.03166103,  0.02115512]),\n",
       " 'score_time': array([ 0.00131989,  0.00040197]),\n",
       " 'test_r2': array([-18.09307872,  -0.38404535]),\n",
       " 'train_r2': array([ 0.99971516,  0.99547953])}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(dt_reg, out_file='tree.dot', feature_names=predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: dot: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!dot -Tpng tree.dot > tree.png # to convert the tree in a png image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[num_vars] = (data_test[num_vars] - data_test[num_vars].mean())/data_test[num_vars].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cancer_volume                  -8.698655e-17\n",
       "weight                         -1.241847e-16\n",
       "age                             5.036063e-16\n",
       "benign_prostatic_hyperplasia    3.605363e-17\n",
       "capsular_penetration           -2.655379e-16\n",
       "gleason_score                  -5.081846e-16\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[num_vars].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rescaled = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_number</th>\n",
       "      <th>psa_level</th>\n",
       "      <th>cancer_volume</th>\n",
       "      <th>weight</th>\n",
       "      <th>age</th>\n",
       "      <th>benign_prostatic_hyperplasia</th>\n",
       "      <th>seminal_vesicle_invasion</th>\n",
       "      <th>capsular_penetration</th>\n",
       "      <th>gleason_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-0.817014</td>\n",
       "      <td>-0.646151</td>\n",
       "      <td>-1.862426</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.852</td>\n",
       "      <td>-0.840908</td>\n",
       "      <td>-0.390140</td>\n",
       "      <td>-0.787896</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.852</td>\n",
       "      <td>-0.811863</td>\n",
       "      <td>-0.672997</td>\n",
       "      <td>1.361163</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.852</td>\n",
       "      <td>-0.849841</td>\n",
       "      <td>-0.413857</td>\n",
       "      <td>-0.787896</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.448</td>\n",
       "      <td>-0.619435</td>\n",
       "      <td>-0.319754</td>\n",
       "      <td>-0.250631</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2.160</td>\n",
       "      <td>-0.843661</td>\n",
       "      <td>-0.442213</td>\n",
       "      <td>-1.862426</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2.160</td>\n",
       "      <td>-0.622112</td>\n",
       "      <td>-0.292186</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>-0.222958</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2.340</td>\n",
       "      <td>-0.635080</td>\n",
       "      <td>-0.241207</td>\n",
       "      <td>-0.787896</td>\n",
       "      <td>0.702656</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2.858</td>\n",
       "      <td>-0.829894</td>\n",
       "      <td>-0.241207</td>\n",
       "      <td>-2.265375</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2.858</td>\n",
       "      <td>-0.729943</td>\n",
       "      <td>-0.436655</td>\n",
       "      <td>-0.116315</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>3.561</td>\n",
       "      <td>-0.725134</td>\n",
       "      <td>-0.194582</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>3.561</td>\n",
       "      <td>-0.855170</td>\n",
       "      <td>-0.194582</td>\n",
       "      <td>-0.116315</td>\n",
       "      <td>0.338540</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>3.561</td>\n",
       "      <td>-0.253257</td>\n",
       "      <td>-0.546993</td>\n",
       "      <td>-0.116315</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.448432</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3.857</td>\n",
       "      <td>-0.330647</td>\n",
       "      <td>-0.555855</td>\n",
       "      <td>0.420950</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>4.055</td>\n",
       "      <td>-0.462536</td>\n",
       "      <td>-0.312971</td>\n",
       "      <td>-0.922212</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.421551</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>4.263</td>\n",
       "      <td>-0.296171</td>\n",
       "      <td>-0.528680</td>\n",
       "      <td>0.286634</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>4.349</td>\n",
       "      <td>-0.804693</td>\n",
       "      <td>-0.256150</td>\n",
       "      <td>0.823898</td>\n",
       "      <td>0.303801</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.448432</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>4.437</td>\n",
       "      <td>0.364962</td>\n",
       "      <td>-0.153514</td>\n",
       "      <td>0.286634</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.210837</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>4.759</td>\n",
       "      <td>-0.815580</td>\n",
       "      <td>-0.419655</td>\n",
       "      <td>-3.071272</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>4.953</td>\n",
       "      <td>-0.736148</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.823898</td>\n",
       "      <td>0.898851</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>5.155</td>\n",
       "      <td>-0.487317</td>\n",
       "      <td>-0.326493</td>\n",
       "      <td>-0.653580</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>5.259</td>\n",
       "      <td>0.107516</td>\n",
       "      <td>-0.270788</td>\n",
       "      <td>-0.519264</td>\n",
       "      <td>0.598604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.426089</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>5.474</td>\n",
       "      <td>-0.814121</td>\n",
       "      <td>-0.352704</td>\n",
       "      <td>-0.653580</td>\n",
       "      <td>-0.687992</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>5.529</td>\n",
       "      <td>-0.135617</td>\n",
       "      <td>-0.306123</td>\n",
       "      <td>-0.116315</td>\n",
       "      <td>-0.323975</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266705</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>5.641</td>\n",
       "      <td>-0.700644</td>\n",
       "      <td>-0.136514</td>\n",
       "      <td>0.689582</td>\n",
       "      <td>0.797801</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>5.871</td>\n",
       "      <td>-0.347117</td>\n",
       "      <td>-0.499843</td>\n",
       "      <td>0.555266</td>\n",
       "      <td>-0.390880</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>6.050</td>\n",
       "      <td>-0.676751</td>\n",
       "      <td>-0.092492</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.474732</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>6.172</td>\n",
       "      <td>-0.803006</td>\n",
       "      <td>0.053619</td>\n",
       "      <td>0.420950</td>\n",
       "      <td>1.199922</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>6.360</td>\n",
       "      <td>-0.529064</td>\n",
       "      <td>-0.494855</td>\n",
       "      <td>0.420950</td>\n",
       "      <td>-0.425124</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315613</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>6.619</td>\n",
       "      <td>0.524729</td>\n",
       "      <td>-0.352704</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>0.742133</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>19.298</td>\n",
       "      <td>0.257119</td>\n",
       "      <td>0.260488</td>\n",
       "      <td>1.092531</td>\n",
       "      <td>2.487377</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.421551</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>19.298</td>\n",
       "      <td>-0.807155</td>\n",
       "      <td>0.804673</td>\n",
       "      <td>0.689582</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>19.492</td>\n",
       "      <td>-0.470961</td>\n",
       "      <td>1.610602</td>\n",
       "      <td>1.092531</td>\n",
       "      <td>2.554512</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.474732</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>20.287</td>\n",
       "      <td>-0.072959</td>\n",
       "      <td>-0.202546</td>\n",
       "      <td>-0.519264</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>0.395956</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>20.905</td>\n",
       "      <td>-0.483295</td>\n",
       "      <td>-0.377909</td>\n",
       "      <td>1.764112</td>\n",
       "      <td>1.062253</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>21.328</td>\n",
       "      <td>-0.462536</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.689582</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.264124</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>21.758</td>\n",
       "      <td>-0.089100</td>\n",
       "      <td>-0.436655</td>\n",
       "      <td>-0.519264</td>\n",
       "      <td>-0.323975</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266705</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>26.576</td>\n",
       "      <td>1.660581</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.689582</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>1.191473</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>28.219</td>\n",
       "      <td>2.043584</td>\n",
       "      <td>-0.425366</td>\n",
       "      <td>0.555266</td>\n",
       "      <td>-0.522413</td>\n",
       "      <td>1</td>\n",
       "      <td>2.378998</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>29.666</td>\n",
       "      <td>0.058955</td>\n",
       "      <td>0.841037</td>\n",
       "      <td>1.092531</td>\n",
       "      <td>1.912253</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.157710</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>31.187</td>\n",
       "      <td>0.720862</td>\n",
       "      <td>0.699849</td>\n",
       "      <td>1.898428</td>\n",
       "      <td>2.554512</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>31.817</td>\n",
       "      <td>0.907935</td>\n",
       "      <td>-0.210422</td>\n",
       "      <td>0.689582</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>2.894735</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>33.448</td>\n",
       "      <td>1.157273</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>-0.116315</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.210837</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>33.784</td>\n",
       "      <td>-0.336192</td>\n",
       "      <td>-0.523998</td>\n",
       "      <td>0.286634</td>\n",
       "      <td>-0.258654</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.264124</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>34.124</td>\n",
       "      <td>0.673304</td>\n",
       "      <td>-0.292186</td>\n",
       "      <td>-0.922212</td>\n",
       "      <td>-0.323975</td>\n",
       "      <td>0</td>\n",
       "      <td>2.123139</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>35.517</td>\n",
       "      <td>0.837524</td>\n",
       "      <td>0.074820</td>\n",
       "      <td>1.764112</td>\n",
       "      <td>-0.642036</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.130749</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>35.517</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.022615</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>0.174907</td>\n",
       "      <td>0</td>\n",
       "      <td>0.927552</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>36.234</td>\n",
       "      <td>-0.284218</td>\n",
       "      <td>-0.101463</td>\n",
       "      <td>-0.519264</td>\n",
       "      <td>0.969649</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>37.713</td>\n",
       "      <td>2.552246</td>\n",
       "      <td>-0.256150</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>2.123139</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>39.646</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.083412</td>\n",
       "      <td>-0.787896</td>\n",
       "      <td>0.864508</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>40.854</td>\n",
       "      <td>-0.172314</td>\n",
       "      <td>-0.359093</td>\n",
       "      <td>-0.250631</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.236688</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>53.517</td>\n",
       "      <td>1.219563</td>\n",
       "      <td>1.458846</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>2.500294</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>54.055</td>\n",
       "      <td>-0.284218</td>\n",
       "      <td>-0.110368</td>\n",
       "      <td>1.629796</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>56.261</td>\n",
       "      <td>2.384460</td>\n",
       "      <td>0.324880</td>\n",
       "      <td>0.555266</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>-1.184784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>62.178</td>\n",
       "      <td>0.704848</td>\n",
       "      <td>-0.127893</td>\n",
       "      <td>-0.384948</td>\n",
       "      <td>0.436357</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.593490</td>\n",
       "      <td>0.167264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>80.640</td>\n",
       "      <td>1.262147</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>0.555266</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>0.395956</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>107.770</td>\n",
       "      <td>4.898637</td>\n",
       "      <td>0.085563</td>\n",
       "      <td>-2.668323</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>1.721482</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>170.716</td>\n",
       "      <td>1.441227</td>\n",
       "      <td>-0.339730</td>\n",
       "      <td>-1.593794</td>\n",
       "      <td>-0.836218</td>\n",
       "      <td>1</td>\n",
       "      <td>2.500294</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>239.847</td>\n",
       "      <td>1.372389</td>\n",
       "      <td>-0.046195</td>\n",
       "      <td>0.555266</td>\n",
       "      <td>0.733734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664344</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>265.072</td>\n",
       "      <td>3.189752</td>\n",
       "      <td>0.163956</td>\n",
       "      <td>0.555266</td>\n",
       "      <td>-0.323975</td>\n",
       "      <td>1</td>\n",
       "      <td>4.210243</td>\n",
       "      <td>1.519311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_number  psa_level  cancer_volume    weight       age  \\\n",
       "0           1      0.651      -0.817014 -0.646151 -1.862426   \n",
       "1           2      0.852      -0.840908 -0.390140 -0.787896   \n",
       "2           3      0.852      -0.811863 -0.672997  1.361163   \n",
       "3           4      0.852      -0.849841 -0.413857 -0.787896   \n",
       "4           5      1.448      -0.619435 -0.319754 -0.250631   \n",
       "5           6      2.160      -0.843661 -0.442213 -1.862426   \n",
       "6           7      2.160      -0.622112 -0.292186  0.018001   \n",
       "7           8      2.340      -0.635080 -0.241207 -0.787896   \n",
       "8           9      2.858      -0.829894 -0.241207 -2.265375   \n",
       "9          10      2.858      -0.729943 -0.436655 -0.116315   \n",
       "10         11      3.561      -0.725134 -0.194582  0.152317   \n",
       "11         12      3.561      -0.855170 -0.194582 -0.116315   \n",
       "12         13      3.561      -0.253257 -0.546993 -0.116315   \n",
       "13         14      3.857      -0.330647 -0.555855  0.420950   \n",
       "14         15      4.055      -0.462536 -0.312971 -0.922212   \n",
       "15         16      4.263      -0.296171 -0.528680  0.286634   \n",
       "16         17      4.349      -0.804693 -0.256150  0.823898   \n",
       "17         18      4.437       0.364962 -0.153514  0.286634   \n",
       "18         19      4.759      -0.815580 -0.419655 -3.071272   \n",
       "19         20      4.953      -0.736148  0.012507  0.823898   \n",
       "20         21      5.155      -0.487317 -0.326493 -0.653580   \n",
       "21         22      5.259       0.107516 -0.270788 -0.519264   \n",
       "22         23      5.474      -0.814121 -0.352704 -0.653580   \n",
       "23         24      5.529      -0.135617 -0.306123 -0.116315   \n",
       "24         25      5.641      -0.700644 -0.136514  0.689582   \n",
       "25         26      5.871      -0.347117 -0.499843  0.555266   \n",
       "26         27      6.050      -0.676751 -0.092492  0.152317   \n",
       "27         28      6.172      -0.803006  0.053619  0.420950   \n",
       "28         29      6.360      -0.529064 -0.494855  0.420950   \n",
       "29         30      6.619       0.524729 -0.352704  0.152317   \n",
       "..        ...        ...            ...       ...       ...   \n",
       "67         68     19.298       0.257119  0.260488  1.092531   \n",
       "68         69     19.298      -0.807155  0.804673  0.689582   \n",
       "69         70     19.492      -0.470961  1.610602  1.092531   \n",
       "70         71     20.287      -0.072959 -0.202546 -0.519264   \n",
       "71         72     20.905      -0.483295 -0.377909  1.764112   \n",
       "72         73     21.328      -0.462536  0.012507  0.689582   \n",
       "73         74     21.758      -0.089100 -0.436655 -0.519264   \n",
       "74         75     26.576       1.660581  0.032855  0.689582   \n",
       "75         76     28.219       2.043584 -0.425366  0.555266   \n",
       "76         77     29.666       0.058955  0.841037  1.092531   \n",
       "77         78     31.187       0.720862  0.699849  1.898428   \n",
       "78         79     31.817       0.907935 -0.210422  0.689582   \n",
       "79         80     33.448       1.157273  0.002464 -0.116315   \n",
       "80         81     33.784      -0.336192 -0.523998  0.286634   \n",
       "81         82     34.124       0.673304 -0.292186 -0.922212   \n",
       "82         83     35.517       0.837524  0.074820  1.764112   \n",
       "83         84     35.517       0.962637  0.022615  0.152317   \n",
       "84         85     36.234      -0.284218 -0.101463 -0.519264   \n",
       "85         86     37.713       2.552246 -0.256150  0.018001   \n",
       "86         87     39.646       0.068472 -0.083412 -0.787896   \n",
       "87         88     40.854      -0.172314 -0.359093 -0.250631   \n",
       "88         89     53.517       1.219563  1.458846  0.152317   \n",
       "89         90     54.055      -0.284218 -0.110368  1.629796   \n",
       "90         91     56.261       2.384460  0.324880  0.555266   \n",
       "91         92     62.178       0.704848 -0.127893 -0.384948   \n",
       "92         93     80.640       1.262147  0.064164  0.555266   \n",
       "93         94    107.770       4.898637  0.085563 -2.668323   \n",
       "94         95    170.716       1.441227 -0.339730 -1.593794   \n",
       "95         96    239.847       1.372389 -0.046195  0.555266   \n",
       "96         97    265.072       3.189752  0.163956  0.555266   \n",
       "\n",
       "    benign_prostatic_hyperplasia  seminal_vesicle_invasion  \\\n",
       "0                      -0.836218                         0   \n",
       "1                      -0.836218                         0   \n",
       "2                      -0.836218                         0   \n",
       "3                      -0.836218                         0   \n",
       "4                      -0.836218                         0   \n",
       "5                      -0.836218                         0   \n",
       "6                      -0.222958                         0   \n",
       "7                       0.702656                         0   \n",
       "8                      -0.836218                         0   \n",
       "9                      -0.836218                         0   \n",
       "10                     -0.836218                         0   \n",
       "11                      0.338540                         0   \n",
       "12                     -0.836218                         0   \n",
       "13                     -0.836218                         0   \n",
       "14                     -0.836218                         0   \n",
       "15                     -0.836218                         0   \n",
       "16                      0.303801                         0   \n",
       "17                     -0.836218                         0   \n",
       "18                     -0.836218                         0   \n",
       "19                      0.898851                         0   \n",
       "20                     -0.836218                         0   \n",
       "21                      0.598604                         0   \n",
       "22                     -0.687992                         0   \n",
       "23                     -0.323975                         0   \n",
       "24                      0.797801                         0   \n",
       "25                     -0.390880                         0   \n",
       "26                     -0.836218                         0   \n",
       "27                      1.199922                         0   \n",
       "28                     -0.425124                         0   \n",
       "29                     -0.836218                         0   \n",
       "..                           ...                       ...   \n",
       "67                      2.487377                         0   \n",
       "68                     -0.836218                         0   \n",
       "69                      2.554512                         0   \n",
       "70                     -0.836218                         1   \n",
       "71                      1.062253                         0   \n",
       "72                     -0.836218                         1   \n",
       "73                     -0.323975                         1   \n",
       "74                     -0.836218                         1   \n",
       "75                     -0.522413                         1   \n",
       "76                      1.912253                         0   \n",
       "77                      2.554512                         0   \n",
       "78                     -0.836218                         1   \n",
       "79                     -0.836218                         0   \n",
       "80                     -0.258654                         0   \n",
       "81                     -0.323975                         0   \n",
       "82                     -0.642036                         1   \n",
       "83                      0.174907                         0   \n",
       "84                      0.969649                         0   \n",
       "85                     -0.836218                         1   \n",
       "86                      0.864508                         0   \n",
       "87                     -0.836218                         1   \n",
       "88                     -0.836218                         1   \n",
       "89                      0.008338                         1   \n",
       "90                     -0.836218                         0   \n",
       "91                      0.436357                         1   \n",
       "92                     -0.836218                         1   \n",
       "93                     -0.836218                         1   \n",
       "94                     -0.836218                         1   \n",
       "95                      0.733734                         1   \n",
       "96                     -0.323975                         1   \n",
       "\n",
       "    capsular_penetration  gleason_score  \n",
       "0              -0.593490      -1.184784  \n",
       "1              -0.593490       0.167264  \n",
       "2              -0.593490       0.167264  \n",
       "3              -0.593490      -1.184784  \n",
       "4              -0.593490      -1.184784  \n",
       "5              -0.593490      -1.184784  \n",
       "6              -0.593490      -1.184784  \n",
       "7              -0.593490      -1.184784  \n",
       "8              -0.593490       0.167264  \n",
       "9              -0.593490      -1.184784  \n",
       "10             -0.593490      -1.184784  \n",
       "11             -0.593490      -1.184784  \n",
       "12             -0.448432       0.167264  \n",
       "13             -0.593490       0.167264  \n",
       "14             -0.421551       0.167264  \n",
       "15             -0.593490      -1.184784  \n",
       "16             -0.448432       0.167264  \n",
       "17             -0.210837      -1.184784  \n",
       "18             -0.593490      -1.184784  \n",
       "19             -0.593490       0.167264  \n",
       "20             -0.593490      -1.184784  \n",
       "21              0.426089       0.167264  \n",
       "22             -0.593490      -1.184784  \n",
       "23              0.266705       0.167264  \n",
       "24             -0.593490      -1.184784  \n",
       "25             -0.593490      -1.184784  \n",
       "26             -0.474732       0.167264  \n",
       "27             -0.593490       0.167264  \n",
       "28             -0.315613       0.167264  \n",
       "29              0.742133      -1.184784  \n",
       "..                   ...            ...  \n",
       "67             -0.421551       0.167264  \n",
       "68             -0.593490      -1.184784  \n",
       "69             -0.474732       0.167264  \n",
       "70              0.395956       0.167264  \n",
       "71             -0.593490       0.167264  \n",
       "72             -0.264124       0.167264  \n",
       "73              0.266705       1.519311  \n",
       "74              1.191473       1.519311  \n",
       "75              2.378998      -1.184784  \n",
       "76             -0.157710       1.519311  \n",
       "77             -0.593490       1.519311  \n",
       "78              2.894735       0.167264  \n",
       "79             -0.210837       1.519311  \n",
       "80             -0.264124       0.167264  \n",
       "81              2.123139       0.167264  \n",
       "82             -0.130749       0.167264  \n",
       "83              0.927552       1.519311  \n",
       "84              0.000670       1.519311  \n",
       "85              2.123139       1.519311  \n",
       "86             -0.593490      -1.184784  \n",
       "87             -0.236688       0.167264  \n",
       "88              2.500294       1.519311  \n",
       "89              0.000670       1.519311  \n",
       "90             -0.593490      -1.184784  \n",
       "91             -0.593490       0.167264  \n",
       "92              0.395956       1.519311  \n",
       "93              1.721482       1.519311  \n",
       "94              2.500294       1.519311  \n",
       "95              0.664344       1.519311  \n",
       "96              4.210243       1.519311  \n",
       "\n",
       "[97 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
